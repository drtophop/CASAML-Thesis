{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOXkYoXMdRLOINhPP/RUf3k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drtophop/CASAML-Thesis/blob/main/CAS_AML_Thesis_NguyenHop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dcda3e5-ad82-472c-bc72-5310a1e59683"
      },
      "source": [
        "# CAS AML Thesis: Preliminary work for Custom Object detection and segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Author: Dr. sc. ETH Minh Hop Nguyen\n",
        "\n",
        "Date: 15.06.2023\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jqD9CyhkZPv9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction"
      ],
      "metadata": {
        "id": "RP6iW0pm11q6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scope"
      ],
      "metadata": {
        "id": "x9T7lebI6Vq3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Contents"
      ],
      "metadata": {
        "id": "W6SlaM6Y1-Si"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sources"
      ],
      "metadata": {
        "id": "FEqRMsT72Mg6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remarks"
      ],
      "metadata": {
        "id": "C8uZGoGP6ZVH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries, Modules and Initializations"
      ],
      "metadata": {
        "id": "OZ0gVTtB5-yJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Numpy` is a powerful Python library that provides support for large, multi-dimensional arrays and a wide range of mathematical functions for efficient numerical computations and is imported as `pandas`"
      ],
      "metadata": {
        "id": "GtxDtIU5drdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "EJcI1g4rYETg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `pandas` library is imported as `pd` to facilitate data manipulation and analysis tasks."
      ],
      "metadata": {
        "id": "JDC5IXE4d7QP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "VdZj2lbtd2Ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `os` module is imported to provide functionalities for interacting with the operating system, while the `time` and `random` modules are imported for time-related operations and generating random numbers, respectively\n"
      ],
      "metadata": {
        "id": "wA6mE0SkeVwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time, random"
      ],
      "metadata": {
        "id": "dNQggfP-XvBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "with `PyTorch` neural networks can be build and trained a more organized and simplified manner"
      ],
      "metadata": {
        "id": "wddzgKdYeu6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pytorch-lightning"
      ],
      "metadata": {
        "id": "jvADo7qM5SbM",
        "outputId": "de50dd37-6f62-4d8f-9f23-d62b91de64e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch-lightning\n",
            "  Using cached pytorch_lightning-2.0.3-py3-none-any.whl (720 kB)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.22.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.65.0)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2023.4.0)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n",
            "  Using cached torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (23.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.5.0)\n",
            "Collecting lightning-utilities>=0.7.0 (from pytorch-lightning)\n",
            "  Using cached lightning_utilities-0.8.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.27.1)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]>2021.06.0->pytorch-lightning)\n",
            "  Using cached aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (3.12.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch-lightning) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch-lightning) (16.0.5)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning)\n",
            "  Using cached multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning)\n",
            "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->pytorch-lightning) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->pytorch-lightning) (1.3.0)\n",
            "Installing collected packages: multidict, lightning-utilities, frozenlist, async-timeout, yarl, aiosignal, aiohttp, torchmetrics, pytorch-lightning\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 lightning-utilities-0.8.0 multidict-6.0.4 pytorch-lightning-2.0.3 torchmetrics-0.11.4 yarl-1.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the main PyTorch library, allowing you to use its functionalities for\n",
        "# tensor computations and neural network operations.\n",
        "import torch\n",
        "\n",
        "# Imports the nn module from PyTorch, which provides classes for defining and\n",
        "# building neural network architectures\n",
        "from torch import nn\n",
        "\n",
        "# These imports the Dataset, DataLoader, and random_split classes from\n",
        "# the torch.utils.data module, which are useful for handling datasets and\n",
        "# creating data loaders for training and validation.\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "\n",
        "# Imports the F module from PyTorch, which contains various functional\n",
        "# operations commonly used in neural networks, such as activation functions.\n",
        "from torch.functional import F\n",
        "\n",
        "\n",
        "# These import the transforms module from torchvision, which provides common\n",
        "# image transformations and in addition specific transformations: ToTensor,\n",
        "# Resize, and ToPILImage.\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor, Resize, ToPILImage\n",
        "\n",
        "# pytorch_lightning library as pl is a lightweight PyTorch wrapper for\n",
        "# high-level training and research purposes.\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "# the train_test_split function from the sklearn.model_selection module can be\n",
        "# used to split a dataset into training and validation sets.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# checks if a CUDA-enabled GPU is available and prints True if it is\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "GkQHhQ0sX08x",
        "outputId": "2d3f5002-7033-4055-e6a2-7085899efbbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Displaying and managing output in an `IPython` environment and working with file paths"
      ],
      "metadata": {
        "id": "HTijKg2dg87x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython import display\n",
        "from IPython.display import clear_output\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "w43wbFBYX3RJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modules for working with images and displaying plots and for for image I/O operations"
      ],
      "metadata": {
        "id": "x7jZS1W_hhit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# imports the pyplot module from Matplotlib as plt for creating and displaying\n",
        "# plots\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# imports functions for reading and manipulating image files as mpimg\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "# imports the io module from the scikit-image library, which provides functions\n",
        "# for reading and saving various image formats\n",
        "import skimage.io\n",
        "\n",
        "# v3 module from the imageio library, which is a versatile library for reading\n",
        "# and writing a wide range of image and video formats\n",
        "import imageio.v3 as iio\n",
        "\n",
        "# This imports the Image class from the PIL (Python Imaging Library) library,\n",
        "# for opening, manipulating, and saving many different image file formats\n",
        "which provides support for opening, manipulating, and saving many different image file formats\n",
        "from PIL import Image\n"
      ],
      "metadata": {
        "id": "GghZXjVcX9H7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modules for working with YAML files, file globbing, input/output operations, image processing using OpenCV, and JSON file handling"
      ],
      "metadata": {
        "id": "7nXVBlVJjRqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# yaml module provides functions for working with YAML (YAML Ain't Markup\n",
        "# Language) files\n",
        "import yaml\n",
        "\n",
        "# glob module provides functions for file globbing, allowing to search for files\n",
        "# using wildcard patterns in a specified directory\n",
        "import glob\n",
        "\n",
        "# io module offers functionalities to create, read, and write data to and from\n",
        "# input/output streams.\n",
        "import io\n",
        "\n",
        "# cv2 module from OpenCV (Open Source Computer Vision Library) is a popular\n",
        "# library for computer vision and image processing tasks\n",
        "import cv2\n",
        "\n",
        "# json module, which provides functions for working with JSON (JavaScript\n",
        "# Object Notation) data.\n",
        "import json"
      ],
      "metadata": {
        "id": "qE_thxM5X_Hz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shutil module, which provides functions for high-level file operations, such\n",
        "# as copying, moving, and deleting files and directories\n",
        "import shutil\n",
        "\n",
        "# The tqdm library is used for creating progress bars in loops and iterable\n",
        "# processes.\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "KWcdBGLTYBgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Opendatasets"
      ],
      "metadata": {
        "id": "B24N-dSK-_QG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The opendatasets Python package, provides a convenient way to download and work with open datasets from various sources"
      ],
      "metadata": {
        "id": "WsQC_QUK_VXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opendatasets"
      ],
      "metadata": {
        "id": "zVSvlT_u-8ue",
        "outputId": "d7e7e19b-4cae-4737-ba48-eb049ab4520d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatasets) (4.65.0)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (from opendatasets) (1.5.13)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from opendatasets) (8.1.3)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2022.12.7)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.27.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (1.26.15)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.4)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import opendatasets as od"
      ],
      "metadata": {
        "id": "rnNKpaOYaWz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Inititals Folders"
      ],
      "metadata": {
        "id": "EP_BIGwi7i0O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "this imports modules for displaying HTML content and setting up a local folder path, retrieves a Python script from a URL, and uses a helper function to set the data path based on the runtime environment (Colab or local)."
      ],
      "metadata": {
        "id": "tmzO41kAlBqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "\n",
        "# set path containing data folder or use default for Colab (/gdrive/My Drive)\n",
        "local_folder = '/gdrive/My Drive/Colab Notebooks/Thesis';\n",
        "\n",
        "\n",
        "import urllib.request\n",
        "urllib.request.urlretrieve('https://raw.githubusercontent.com/guiwitz/MLCV/main/notebooks/check_colab.py', 'check_colab.py')\n",
        "from check_colab import set_datapath\n",
        "colab, datapath = set_datapath(local_folder)\n",
        "\n"
      ],
      "metadata": {
        "id": "ymImqprQ1Iq0",
        "outputId": "16b607bf-3571-4d20-ad4f-e0cce9f89aff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-03d5d91e7541>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://raw.githubusercontent.com/guiwitz/MLCV/main/notebooks/check_colab.py'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'check_colab.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcheck_colab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_datapath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mcolab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatapath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_datapath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/check_colab.py\u001b[0m in \u001b[0;36mset_datapath\u001b[0;34m(default_path)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mIS_COLAB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mdatapath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/gdrive/My Drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    130\u001b[0m   )\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Local Folders"
      ],
      "metadata": {
        "id": "83UTDwPdv6b5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieves the current working directory as `path_main`"
      ],
      "metadata": {
        "id": "tVRVnn8OlR3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_main = os.getcwd()"
      ],
      "metadata": {
        "id": "8MAJMQIK7oev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the String for the detection dataset folder"
      ],
      "metadata": {
        "id": "qTW0Ib8klc1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_yoloset = f'{path_main}/car-object-detection/data'"
      ],
      "metadata": {
        "id": "UsCXoMQZ8FJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Clear folders"
      ],
      "metadata": {
        "id": "T7bmbKBg7LsW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before starting with the notebook, existing contents and existing folders from a potentional former execution of this Notebook will be cleared to show proper execution, by following code snippet"
      ],
      "metadata": {
        "id": "eupjOGmu8hlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dirs_to_delete = ['car-object-detection', 'yolov5', 'test', 'tmp', 'train', 'valid', 'yolov5_train']\n",
        "\n",
        "for dir_to_delete in dirs_to_delete:\n",
        "    path_del = os.path.join(path_main, dir_to_delete)\n",
        "    if os.path.exists(path_del):\n",
        "        shutil.rmtree(path_del)\n",
        "\n",
        "files_to_delete = ['data.yaml','yolov5m6.pt']\n",
        "\n",
        "for file_to_delete in files_to_delete:\n",
        "    file_del = os.path.join(path_main, file_to_delete)\n",
        "    if os.path.exists(file_del):\n",
        "        os.remove(file_del)\n",
        "\n",
        "\n",
        "dirs_to_delete = ['carvana-image-masking-png']\n",
        "\n",
        "for dir_to_delete in dirs_to_delete:\n",
        "    path_del = os.path.join(path_main, dir_to_delete)\n",
        "    if os.path.exists(path_del):\n",
        "        shutil.rmtree(path_del)"
      ],
      "metadata": {
        "id": "a2hEQs2H9SA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datasets"
      ],
      "metadata": {
        "id": "93Mpzn8x6Iou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Order to train Object recognition and Object segmentation, Datasets will be downloaded from [https://www.kaggle.com/](https://www.kaggle.com/)."
      ],
      "metadata": {
        "id": "qS20lensmOwg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Object Recognition"
      ],
      "metadata": {
        "id": "PWvWM4q86Pie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "text"
      ],
      "metadata": {
        "id": "J4g0W4tgl8y9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "od.download('https://www.kaggle.com/datasets/sshikamaru/car-object-detection/download?datasetVersionNumber=2')"
      ],
      "metadata": {
        "id": "Eg7dqn5DYSMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Object Segmentation"
      ],
      "metadata": {
        "id": "NjiVGRsp6fBt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "text"
      ],
      "metadata": {
        "id": "1OAIEqwym1f7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "od.download('https://www.kaggle.com/datasets/ipythonx/carvana-image-masking-png/download?datasetVersionNumber=1')"
      ],
      "metadata": {
        "id": "YXahwPC_0lHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Yolov5 Object Recoginition"
      ],
      "metadata": {
        "id": "4ZnMlbXc7CdW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOLOv5 is an object detection model that belongs to the YOLO (You Only Look Once) family, known for its real-time and high-performance capabilities in detecting objects in images and videos.\n",
        "It improves upon previous versions by introducing a more streamlined architecture, including various sizes of models (small, medium, large, and extra-large), achieving better accuracy and speed trade-offs.\n",
        "YOLOv5 utilizes a deep neural network architecture with anchor-based bounding box predictions and employs advanced techniques like focal loss, focal loss gradient scaler, and weighted box fusion for improved object detection results.\n",
        "\n",
        "In this thesis yolov5 will be trained with custom data, therefore the workflow will be used"
      ],
      "metadata": {
        "id": "4KZw-WRvnGSS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clone and Setup Yolo V5"
      ],
      "metadata": {
        "id": "FmFuOUmQ9rfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone yolov5 from the corresponding Git Repository to Local\n",
        "!git clone https://github.com/ultralytics/yolov5"
      ],
      "metadata": {
        "id": "fStg9AF49o0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the required Python packages specified in the requirements.txt file within the yolov5 directory\n",
        "!pip install -qr yolov5/requirements.txt"
      ],
      "metadata": {
        "id": "rtlMtyfV-Yp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialization"
      ],
      "metadata": {
        "id": "yuxoMnhzAjVz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up the Hyperparameters to run yolov5"
      ],
      "metadata": {
        "id": "jyuUu1nuBN8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_NAME = \"yolov5_train\"\n",
        "BASE_MODEL = \"yolov5m6.pt\"\n",
        "TRAIN_BATCH = 32\n",
        "TRAIN_EPOCHS = 20\n",
        "VAL_BATCH = 64\n",
        "\n",
        "\n",
        "# Doublecheck if needed Folders are ok\n",
        "print(f'does path_main exist: {os.path.exists(path_main)}')\n",
        "print(f'does path_yoloset exist: {os.path.exists(path_yoloset)}')"
      ],
      "metadata": {
        "id": "tfCOZi_k-t8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_h, img_w, num_channels = (380, 676, 3)\n",
        "\n",
        "df = pd.read_csv(f'{path_yoloset}/train_solution_bounding_boxes (1).csv')\n",
        "\n",
        "df.rename(columns={'image':'image_id'}, inplace=True)\n",
        "df['image_id'] = df['image_id'].apply(lambda x: x.split('.')[0])\n",
        "df['x_center'] = (df['xmin'] + df['xmax'])/2\n",
        "df['y_center'] = (df['ymin'] + df['ymax'])/2\n",
        "df['w'] = df['xmax'] - df['xmin']\n",
        "df['h'] = df['ymax'] - df['ymin']\n",
        "df['classes'] = 0\n",
        "df['x_center'] = df['x_center']/img_w\n",
        "df['w'] = df['w']/img_w\n",
        "df['y_center'] = df['y_center']/img_h\n",
        "df['h'] = df['h']/img_h\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "zJtMkDMLYk2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = list(set(df.image_id))\n",
        "image = random.choice(index)\n",
        "print(\"Image ID: %s\"%(image))\n",
        "\n",
        "img = cv2.imread(f'{path_yoloset}/training_images/{image}.jpg')\n",
        "img.shape"
      ],
      "metadata": {
        "id": "2SbTFZU0Yn_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = cv2.imread(f'{path_yoloset}/training_images/{image}.jpg')\n",
        "\n",
        "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CAVazeKaYoqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source = 'training_images'\n",
        "\n",
        "for name, mini in tqdm(df.groupby('image_id')):\n",
        "    if not os.path.exists(os.path.join(path_main, 'tmp', 'labels')):\n",
        "        os.makedirs(os.path.join(path_main, 'tmp', 'labels'))\n",
        "\n",
        "    with open(os.path.join(path_main, 'tmp', 'labels', '{}.txt'.format(name)), 'w+') as f:\n",
        "        row = mini[['classes', 'x_center', 'y_center', 'w', 'h']].astype(float).values\n",
        "        row = row.astype(str)\n",
        "        for j in range(len(row)):\n",
        "            text = ' '.join(row[j])\n",
        "            f.write(text)\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "    if not os.path.exists(os.path.join(path_main, 'tmp', 'images')):\n",
        "        os.makedirs(os.path.join(path_main, 'tmp', 'images'))\n",
        "\n",
        "    shutil.copy(\n",
        "        os.path.join(path_yoloset, source, '{}.jpg'.format(name)),\n",
        "        os.path.join(path_main, 'tmp', 'images', '{}.jpg'.format(name)))"
      ],
      "metadata": {
        "id": "Rw7QRpbTYrfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGES_PATH = f'{path_main}/tmp/images/'\n",
        "LABELS_PATH = f'{path_main}/tmp/labels/'\n",
        "os.path.exists(IMAGES_PATH)"
      ],
      "metadata": {
        "id": "LbyLyCKQYupZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read labels\n",
        "labels = os.listdir(LABELS_PATH)\n",
        "\n",
        "\n",
        "# Split data\n",
        "train, test = train_test_split(labels, test_size=0.15, shuffle=True)\n",
        "valid, test = train_test_split(test, test_size=0.2)\n",
        "\n",
        "print(f\"train: {len(train)}; valid: {len(valid)}; test: {len(test)}\")"
      ],
      "metadata": {
        "id": "hlEZksLsYyLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(f'{path_main}/test/images')\n",
        "os.makedirs(f'{path_main}/test/labels')\n",
        "os.makedirs(f'{path_main}/train/images')\n",
        "os.makedirs(f'{path_main}/train/labels')\n",
        "os.makedirs(f'{path_main}/valid/images')\n",
        "os.makedirs(f'{path_main}/valid/labels')"
      ],
      "metadata": {
        "id": "9ZqThILLY0px"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to move files to directory\n",
        "def move_files_to_dir(files, dirname):\n",
        "    # Create target directories if they don't exist\n",
        "    os.makedirs(f'{path_main}/{dirname}/images', exist_ok=True)\n",
        "    os.makedirs(f'{path_main}/{dirname}/labels', exist_ok=True)\n",
        "\n",
        "    for label_filename in files:\n",
        "        image_filename = f\"{label_filename[:-4]}.jpg\"\n",
        "        shutil.copy(f\"{IMAGES_PATH}/{image_filename}\", f\"{path_main}/{dirname}/images/{image_filename}\")\n",
        "        shutil.copy(f\"{LABELS_PATH}/{label_filename}\", f\"{path_main}/{dirname}/labels/{label_filename}\")\n",
        "\n",
        "\n",
        "# Move splits to folders\n",
        "move_files_to_dir(train, \"train\")\n",
        "move_files_to_dir(test, \"test\")\n",
        "move_files_to_dir(valid, \"valid\")\n"
      ],
      "metadata": {
        "id": "WBTGFMBjY2d6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "directories = ['train', 'test', 'valid']\n",
        "subdirectories = ['images', 'labels']\n",
        "\n",
        "for directory in directories:\n",
        "    for subdirectory in subdirectories:\n",
        "        path = os.path.join(path_main, directory, subdirectory)\n",
        "        file_count = len(os.listdir(path))\n",
        "        print(f\"Number of files in {path}: {file_count}\")"
      ],
      "metadata": {
        "id": "fzGlWjy2Y45R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = f'{path_main}/train'\n",
        "test_path = f'{path_main}/test'\n",
        "valid_path  = f'{path_main}/valid'\n",
        "\n",
        "test_path"
      ],
      "metadata": {
        "id": "DVXEFebbY6uB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "    f\"train: {train_path}\\n\"\n",
        "    f\"test: {test_path}\\n\"\n",
        "    f\"val: {valid_path}\\n\"\n",
        "    f\"nc: {1}\\n\"\n",
        "    f\"names: HopCar\",\n",
        ")\n",
        "\n",
        "with open(\"data.yaml\", \"w\") as file:\n",
        "    yaml.dump({\n",
        "        \"train\": train_path,\n",
        "        \"test\": test_path,\n",
        "        \"val\": valid_path,\n",
        "        \"nc\": 1,\n",
        "        \"names\": {0: \"HopCar\"}\n",
        "    }, file)"
      ],
      "metadata": {
        "id": "2cPwyh_iY_Jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete old results if exists\n",
        "wildcard = f\"{PROJECT_NAME}/feature_extraction*\"\n",
        "! rm -r $wildcard"
      ],
      "metadata": {
        "id": "UVZ4B3MKZBRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python yolov5/train.py --batch $TRAIN_BATCH --epochs $TRAIN_EPOCHS --data \"data.yaml\" --weights $BASE_MODEL --project $PROJECT_NAME --name 'feature_extraction' --cache --freeze 12"
      ],
      "metadata": {
        "id": "Sz8kmU5EZC9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete old results\n",
        "wildcard = f\"{PROJECT_NAME}/validation_on_test_data*\"\n",
        "! rm -r $wildcard"
      ],
      "metadata": {
        "id": "GaP3TnjPZG73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WEIGHTS_BEST = f\"{PROJECT_NAME}/feature_extraction/weights/best.pt\"\n",
        "! python yolov5/val.py --weights $WEIGHTS_BEST --batch $VAL_BATCH --data 'data.yaml' --task test --project $PROJECT_NAME --name 'validation_on_test_data' --augment"
      ],
      "metadata": {
        "id": "JjQ9cYUYZKax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete old results\n",
        "wildcard = f\"{PROJECT_NAME}/detect_test*\"\n",
        "! rm -r $wildcard"
      ],
      "metadata": {
        "id": "zbWiW8rDZM1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pics = ['F1_curve.png', 'PR_curve.png', 'PR_curve.png', 'R_curve.png']\n",
        "\n",
        "fig, axs = plt.subplots(2, 2,figsize=(10, 10))\n",
        "\n",
        "for i, pic in enumerate(pics):\n",
        "    image_path = f'{PROJECT_NAME}/feature_extraction/{pic}'\n",
        "    img = mpimg.imread(image_path)\n",
        "    row = i // 2\n",
        "    col = i % 2\n",
        "    axs[row, col].imshow(img)\n",
        "    axs[row, col].axis('off')\n",
        "    axs[row, col].set_title(pic[:-4].replace(\"_\", \" \").upper())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KmlqX2DKZOhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pics = ['confusion_matrix.png', 'labels.jpg', 'labels_correlogram.jpg', 'results.png']\n",
        "\n",
        "fig, axs = plt.subplots(4, 1, figsize=(8, 24))  # Adjust figsize as desired\n",
        "\n",
        "for i, pic in enumerate(pics):\n",
        "    image_path = f'{PROJECT_NAME}/feature_extraction/{pic}'\n",
        "    img = mpimg.imread(image_path)\n",
        "    axs[i].imshow(img)\n",
        "    axs[i].axis('off')\n",
        "    axs[i].set_title(pic[:-4].replace(\"_\", \" \").upper())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MwPsMIC3ZQhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pics = ['val_batch0_labels.jpg', 'val_batch0_pred.jpg']\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize=(20, 10))  # Adjust figsize as desired\n",
        "\n",
        "for i, pic in enumerate(pics):\n",
        "    image_path = f'{PROJECT_NAME}/feature_extraction/{pic}'\n",
        "    img = mpimg.imread(image_path)\n",
        "    axs[i].imshow(img)\n",
        "    axs[i].axis('off')\n",
        "    axs[i].set_title(pic[:-4].replace(\"_\", \" \").upper())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vvg9OXiaZSXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python yolov5/detect.py --weights $WEIGHTS_BEST --conf 0.6 --source '/content/tmp/images' --project $PROJECT_NAME --name 'detect_test' --augment --line=3 --save-txt"
      ],
      "metadata": {
        "id": "t4LtuCsuZXTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "directory = \"yolov5_train/detect_test\"\n",
        "jpg_files = [file for file in os.listdir(directory) if file.endswith(\".jpg\")]\n",
        "random_files = random.sample(jpg_files, 5)\n",
        "\n",
        "for file in random_files:\n",
        "    image_path = os.path.join(directory, file)\n",
        "    image = mpimg.imread(image_path)\n",
        "    plt.imshow(image)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ChJZuFHLZhDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the output folder for cropped images\n",
        "output_folder = 'cropped_images'\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Path to the directory containing the labeled and image images\n",
        "label_folder = 'yolov5_train/detect_test/labels'\n",
        "image_folder = \"/content/tmp/images\"\n",
        "\n",
        "\n",
        "# Process each labeled image\n",
        "for label_file in os.listdir(label_folder):\n",
        "    image_name = label_file[:-4] + \".jpg\"\n",
        "    image_path = os.path.join(image_folder, image_name)\n",
        "    label_path = os.path.join(label_folder, label_file)\n",
        "    image = cv2.imread(image_path)\n",
        "    height, width, _ = image.shape\n",
        "\n",
        "    # Read the label file\n",
        "    with open(label_path, \"r\") as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    # Process each line in the label file\n",
        "    for line in lines:\n",
        "        class_id, x_center, y_center, box_width, box_height = map(float, line.strip().split())\n",
        "\n",
        "        # Calculate bounding box coordinates\n",
        "        x_min = int((x_center - box_width / 2) * width)\n",
        "        y_min = int((y_center - box_height / 2) * height)\n",
        "        x_max = int((x_center + box_width / 2) * width)\n",
        "        y_max = int((y_center + box_height / 2) * height)\n",
        "\n",
        "        # Crop and save the image using the bounding box coordinates\n",
        "        cropped_img = image[y_min:y_max, x_min:x_max]\n",
        "        cropped_img_name = f'{image_name[:-4]}_{int(class_id)}.jpg'\n",
        "        cropped_img_path = os.path.join(output_folder, cropped_img_name)\n",
        "        cv2.imwrite(cropped_img_path, cropped_img)"
      ],
      "metadata": {
        "id": "oPqWISfVZk3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Path to the output folder for cropped images\n",
        "output_folder = \"cropped_images\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Path to the directory containing the labeled images\n",
        "label_folder = \"yolov5_train/detect_test/labels\"\n",
        "#image_folder = \"/content/tmp/images\"\n",
        "image_folder = 'yolov5_train/detect_test'\n",
        "\n",
        "# Randomly select 5 images\n",
        "image_files = os.listdir(label_folder)\n",
        "random_images = random.sample(image_files, 5)\n",
        "\n",
        "# Create a figure to display the images\n",
        "fig, axs = plt.subplots(5, 2, figsize=(10, 20))\n",
        "\n",
        "# Process each randomly selected image\n",
        "for i, image_file in enumerate(random_images):\n",
        "    image_name = image_file[:-4] + \".jpg\"\n",
        "    image_path = os.path.join(image_folder, image_name)\n",
        "    label_path = os.path.join(label_folder, image_file)\n",
        "    image = cv2.imread(image_path)\n",
        "    height, width, _ = image.shape\n",
        "\n",
        "    # Read the label file\n",
        "    with open(label_path, \"r\") as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    # Process the first bounding box in the label file\n",
        "    line = lines[0]\n",
        "    class_id, x_center, y_center, box_width, box_height = map(float, line.strip().split())\n",
        "\n",
        "    # Calculate bounding box coordinates\n",
        "    x_min = int((x_center - box_width / 2) * width)\n",
        "    y_min = int((y_center - box_height / 2) * height)\n",
        "    x_max = int((x_center + box_width / 2) * width)\n",
        "    y_max = int((y_center + box_height / 2) * height)\n",
        "\n",
        "    # Crop the image using the bounding box coordinates\n",
        "    cropped_img = image[y_min:y_max, x_min:x_max]\n",
        "\n",
        "    # Display the uncropped image\n",
        "    axs[i, 0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "    axs[i, 0].axis(\"off\")\n",
        "    axs[i, 0].set_title(\"Uncropped Image\")\n",
        "\n",
        "    # Display the cropped image\n",
        "    axs[i, 1].imshow(cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB))\n",
        "    axs[i, 1].axis(\"off\")\n",
        "    axs[i, 1].set_title(\"Cropped Image\")\n",
        "\n",
        "# Adjust the spacing between subplots\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the figure\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tJrGrZCIZoKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "bla"
      ],
      "metadata": {
        "id": "o-GCrzR10yXl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unet Object Segmentation"
      ],
      "metadata": {
        "id": "ati4dCBv0rdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "im_path = f'{path_main}/carvana-image-masking-png/train_images'\n",
        "lab_path = f'{path_main}/carvana-image-masking-png/train_masks'\n",
        "\n",
        "\n",
        "print(f'does im exist: {os.path.exists(im_path)}')\n",
        "print(f'does lab_path exist: {os.path.exists(lab_path)}')"
      ],
      "metadata": {
        "id": "0CjAsYf80xKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_path = datapath.joinpath(im_path)\n",
        "labels_path = datapath.joinpath(lab_path)"
      ],
      "metadata": {
        "id": "FgVhWeeO07zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_files = os.listdir(images_path)\n",
        "images_files_sorted = sorted(images_files)"
      ],
      "metadata": {
        "id": "Sb90fX6i0_E8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# iterate over the sorted list and rename each file with an index-based name\n",
        "for i, file in enumerate(images_files_sorted):\n",
        "    # generate the new name with an index based on the current position in the sorted list\n",
        "    new_name = f\"image_{i}.jpg\"\n",
        "    # use the os.rename() function to rename the file\n",
        "    os.rename(os.path.join(images_path, file), os.path.join(images_path, new_name))"
      ],
      "metadata": {
        "id": "XrcdGyfH1dkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_files = os.listdir(labels_path)\n",
        "labels_files_sorted = sorted(labels_files)"
      ],
      "metadata": {
        "id": "_cjDH9DC1fbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# iterate over the sorted list and rename each file with an index-based name\n",
        "for i, file in enumerate(labels_files_sorted):\n",
        "    # generate the new name with an index based on the current position in the sorted list\n",
        "    new_name = f\"label_{i}.jpg\"\n",
        "    # use the os.rename() function to rename the file\n",
        "    os.rename(os.path.join(labels_path, file), os.path.join(labels_path, new_name))"
      ],
      "metadata": {
        "id": "plIss1uM1h_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(2,2)\n",
        "ax[0,0].imshow(skimage.io.imread(images_path.joinpath('image_20.jpg')))\n",
        "ax[0,1].imshow(skimage.io.imread(images_path.joinpath('image_4.jpg')))\n",
        "ax[1,0].imshow(skimage.io.imread(labels_path.joinpath('label_20.jpg')))\n",
        "ax[1,1].imshow(skimage.io.imread(labels_path.joinpath('label_4.jpg')))\n"
      ],
      "metadata": {
        "id": "BDwdafAT1jpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(2,2)\n",
        "ax[0,0].imshow(skimage.io.imread(images_path.joinpath('image_20.jpg')))\n",
        "ax[0,1].imshow(skimage.io.imread(images_path.joinpath('image_4.jpg')))\n",
        "ax[1,0].imshow(skimage.io.imread(labels_path.joinpath('label_20.jpg')))\n",
        "ax[1,1].imshow(skimage.io.imread(labels_path.joinpath('label_4.jpg')))\n"
      ],
      "metadata": {
        "id": "xVc3dLxy1mEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "im_train = iio.imread(images_path.joinpath('image_20.jpg'))\n",
        "im_label = iio.imread(labels_path.joinpath('label_20.jpg'))\n",
        "print(type(im_train))\n",
        "print(type(im_label))"
      ],
      "metadata": {
        "id": "0ktAeqbB4jzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1,2, figsize=(10, 5))\n",
        "\n",
        "# Plot the histogram of the training image\n",
        "ax[0].hist(im_train.flatten(), bins=256, color='b', alpha=0.5)\n",
        "ax[0].set_xlabel('Pixel Value')\n",
        "ax[0].set_ylabel('Frequency')\n",
        "ax[0].set_title('Histogram of Training Image')\n",
        "\n",
        "# Plot the histogram of the label image\n",
        "ax[1].hist(im_label.flatten(), bins=256, color='r', alpha=0.5)\n",
        "ax[1].set_xlabel('Pixel Value')\n",
        "ax[1].set_ylabel('Frequency')\n",
        "ax[1].set_title('Histogram of Label Image')"
      ],
      "metadata": {
        "id": "AffoqiSv1l-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The resolution of Training Image is\", im_train.shape, \"and data type is\", im_train.dtype, \"since Pixel Values are from 0 to 255\")"
      ],
      "metadata": {
        "id": "8xMd5NOc1quM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import warnings\n",
        "#warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "IqUauZqU6tK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "image_height = 160*3 #1280 #160 #640\n",
        "image_width = 240*3 #1918 #240 #960\n",
        "\n",
        "\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "\n",
        "# Define the augmentations\n",
        "transform = A.Compose([\n",
        "    A.Rotate(limit=40, p=1.0),\n",
        "    A.Resize(image_height, image_width),\n",
        "    A.Normalize(\n",
        "        mean = [0.0,0.0,0.0],\n",
        "        std = [1.0,1.0,1.0],\n",
        "    ),\n",
        "    ToTensorV2()\n",
        "])\n"
      ],
      "metadata": {
        "id": "TcV9NNiN6oeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_height = 160*3 #1280 #160 #640\n",
        "image_width = 240*3 #1918 #240 #960\n",
        "\n",
        "\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "\n",
        "# Define the augmentations\n",
        "transform = A.Compose([\n",
        "    A.Rotate(limit=30, p=1.0),\n",
        "    A.Resize(image_height, image_width),\n",
        "    A.Normalize(\n",
        "        mean = [0.0,0.0,0.0],\n",
        "        std = [1.0,1.0,1.0],\n",
        "    ),\n",
        "    ToTensorV2()\n",
        "])\n"
      ],
      "metadata": {
        "id": "xazfi20c6wy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Segdata(Dataset):\n",
        "    def __init__(self, im_path, label_path, transforms=None):\n",
        "        super(Segdata, self).__init__()\n",
        "        self.im_path = im_path\n",
        "        self.label_path = label_path\n",
        "        self.transform = transforms\n",
        "        self.images = os.listdir(im_path)\n",
        "        self.labels = os.listdir(label_path)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "\n",
        "        image_path = self.im_path.joinpath(f'image_{index}.jpg')\n",
        "        x = np.array(Image.open(image_path).convert(\"RGB\"))\n",
        "\n",
        "        label_path = self.label_path.joinpath(f'label_{index}.jpg')\n",
        "        y = np.array(Image.open(label_path).convert(\"L\"))\n",
        "\n",
        "\n",
        "        y[y == 255.0] = 1.0\n",
        "\n",
        "        if self.transform is not None:\n",
        "\n",
        "            augmentations = self.transform(image = x, mask = y)\n",
        "            x = augmentations['image']\n",
        "            y = augmentations['mask']\n",
        "\n",
        "            y = y.unsqueeze(0)\n",
        "\n",
        "\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)"
      ],
      "metadata": {
        "id": "K3AkM45L6zlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segdata = Segdata(images_path, labels_path, transform)\n",
        "train_size = int(0.8 * len(segdata))\n",
        "valid_size = len(segdata)-train_size\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "train_data, valid_data = random_split(segdata, [train_size, valid_size])\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
        "validation_loader = DataLoader(valid_data, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "SWtRxpGW62uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_test, label_test = segdata[4]"
      ],
      "metadata": {
        "id": "Yu_gkKxG63vC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1,2, figsize=(10, 5))\n",
        "\n",
        "# Plot the histogram of the training image\n",
        "ax[0].hist(image_test.flatten(), bins=256, color='b', alpha=0.5)\n",
        "ax[0].set_xlabel('Pixel Value')\n",
        "ax[0].set_ylabel('Frequency')\n",
        "ax[0].set_title('Histogram of Training Image')\n",
        "\n",
        "# Plot the histogram of the label image\n",
        "ax[1].hist(label_test .flatten(), bins=256, color='r', alpha=0.5)\n",
        "ax[1].set_xlabel('Pixel Value')\n",
        "ax[1].set_ylabel('Frequency')\n",
        "ax[1].set_title('Histogram of Label Image')"
      ],
      "metadata": {
        "id": "l8TdfShu65RI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(image_test.shape)\n",
        "print(label_test.shape)"
      ],
      "metadata": {
        "id": "Bbbqgrmp674s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(image_test.permute(1, 2, 0).shape)\n",
        "print(label_test.permute(1, 2, 0).shape)"
      ],
      "metadata": {
        "id": "rC5wNK6w6-I6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1,2, figsize=(10, 5))\n",
        "\n",
        "ax[0].imshow(image_test.permute(1, 2, 0))\n",
        "ax[1].imshow(label_test.permute(1, 2, 0))"
      ],
      "metadata": {
        "id": "N0oZXp7i6_Fw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader"
      ],
      "metadata": {
        "id": "2-eFbtA57Bqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "im_batch, lab_batch = next(iter(train_loader))"
      ],
      "metadata": {
        "id": "IC-JCc7D7CcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(im_batch.shape)\n",
        "print(lab_batch.shape)"
      ],
      "metadata": {
        "id": "1r3yA4FB7E15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1,2, figsize=(10, 5))\n",
        "\n",
        "# Plot the histogram of the training image\n",
        "ax[0].hist(im_batch[1,:,:,:].flatten(), bins=256, color='b', alpha=0.5)\n",
        "ax[0].set_xlabel('Pixel Value')\n",
        "ax[0].set_ylabel('Frequency')\n",
        "ax[0].set_title('Histogram of Training Image')\n",
        "\n",
        "# Plot the histogram of the label image\n",
        "ax[1].hist(lab_batch[1,:,:,:].flatten(), bins=256, color='r', alpha=0.5)\n",
        "ax[1].set_xlabel('Pixel Value')\n",
        "ax[1].set_ylabel('Frequency')\n",
        "ax[1].set_title('Histogram of Label Image')"
      ],
      "metadata": {
        "id": "JrX_LUWk7Jny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image as IM2\n",
        "IM2(url='https://github.com/guiwitz/DLImaging/raw/master/illustrations/unet.jpg', width=800)"
      ],
      "metadata": {
        "id": "XAcXM1vp1uG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dice_loss(output, target):\n",
        "    smooth = 1e-8  # smoothing factor to avoid division by zero\n",
        "    output = output.view(-1)\n",
        "    target = target.view(-1)\n",
        "\n",
        "    intersection = torch.sum(output * target)\n",
        "    union = torch.sum(output) + torch.sum(target)\n",
        "\n",
        "    dice = (2.0 * intersection + smooth) / (union + smooth)\n",
        "    loss = 1.0 - dice\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "XUleNrHY1w8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Unet(pl.LightningModule):\n",
        "    def __init__(self, learning_rate):\n",
        "        super(Unet, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.transpose_conv3 = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=2, padding=0, stride=2)\n",
        "        self.conv2_t = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.transpose_conv2 = nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=2, padding=0, stride=2)\n",
        "        self.conv1_t = nn.Conv2d(in_channels=32, out_channels=16, kernel_size=3, padding=1)\n",
        "        self.conv_final = nn.Conv2d(in_channels=16, out_channels=1, kernel_size=1)\n",
        "\n",
        "        #self.loss = nn.BCEWithLogitsLoss()\n",
        "        self.loss = dice_loss\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = F.relu(self.conv1(x))\n",
        "        x = self.maxpool1(x1)\n",
        "        x2 = F.relu(self.conv2(x))\n",
        "        x = self.maxpool2(x2)\n",
        "        x3 = F.relu(self.conv3(x))\n",
        "        x2_t = self.transpose_conv3(x3)\n",
        "        x = torch.cat((x2, x2_t), dim=1)\n",
        "        x = F.relu(self.conv2_t(x))\n",
        "        x = self.transpose_conv2(x)\n",
        "        x = torch.cat((x1, x), dim=1)\n",
        "        x = F.relu(self.conv1_t(x))\n",
        "        x = self.conv_final(x)\n",
        "\n",
        "        return x.sigmoid()"
      ],
      "metadata": {
        "id": "VkeWgUWq10nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "if 'unet' in locals():\n",
        "    del unet\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "unet = Unet(learning_rate=0.001)\n",
        "unet = unet.to(device)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "validation_loader = DataLoader(valid_data, batch_size=batch_size)\n",
        "\n",
        "optimizer = torch.optim.Adam(unet.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 2  # Adjust the number of epochs as needed\n",
        "\n",
        "import time\n",
        "\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "\n",
        "train_losses = []\n",
        "validation_losses = []\n",
        "accuracies = []\n",
        "ious = []\n",
        "jaccards = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    unet.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
        "    for batch_idx, (inputs, targets) in progress_bar:\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = unet(inputs)\n",
        "        loss = unet.loss(outputs, targets)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        progress_bar.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "\n",
        "    # Validation\n",
        "    unet.eval()\n",
        "    validation_loss = 0.0\n",
        "    accuracy = 0.0\n",
        "    iou = 0.0\n",
        "    jaccard = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in validation_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            outputs = unet(inputs)\n",
        "            loss = unet.loss(outputs, targets)\n",
        "\n",
        "            validation_loss += loss.item()\n",
        "\n",
        "            outputs_binary = (outputs > 0.5).float()\n",
        "\n",
        "            accuracy += (outputs_binary == targets).float().mean().item()\n",
        "\n",
        "            intersection = (outputs_binary * targets).sum()\n",
        "            union = (outputs_binary + targets).sum()\n",
        "            iou += intersection / (union + 1e-8)\n",
        "\n",
        "            outputs_prob = torch.softmax(outputs, dim=1)\n",
        "            outputs_argmax = outputs_prob.argmax(dim=1).float()\n",
        "            #jaccard += jaccard_score(outputs_binary.view(-1), outputs_argmax.view(-1), average='macro')\n",
        "\n",
        "        validation_loss /= len(validation_loader)\n",
        "        accuracy /= len(validation_loader)\n",
        "        iou /= len(validation_loader)\n",
        "        #jaccard /= len(validation_loader)\n",
        "        #jaccards.append(jaccard)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f} - Validation Loss: {validation_loss:.4f}\"\n",
        "          f\" - Accuracy: {accuracy:.4f} - IoU: {iou:.4f}\")# - Jaccard: {jaccard:.4f}\")\n",
        "\n",
        "    # Append metrics to lists\n",
        "    train_losses.append(train_loss)\n",
        "    validation_losses.append(validation_loss)\n",
        "    accuracies.append(accuracy)\n",
        "    ious.append(iou)\n",
        "\n",
        "    # Save predictions as images for a specific epoch\n",
        "    if epoch == 0 or (epoch + 1) % 5 == 0:\n",
        "        unet.eval()\n",
        "        with torch.no_grad():\n",
        "            example_inputs, example_targets = next(iter(validation_loader))\n",
        "            example_inputs = example_inputs.to(device)\n",
        "            example_targets = example_targets.to(device)\n",
        "\n",
        "            example_outputs = unet(example_inputs)\n",
        "            example_outputs = example_outputs.cpu().detach().numpy()\n",
        "            example_targets = example_targets.cpu().detach().numpy()\n",
        "\n",
        "            # Save example predictions as images\n",
        "            fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
        "            for i, (ax_input, ax_output, ax_target) in enumerate(zip(axes[0], axes[1], axes[2])):\n",
        "              ax_input.imshow(example_inputs[i].cpu().permute(1, 2, 0))\n",
        "              ax_input.set_title(\"Input\")\n",
        "              ax_input.axis(\"off\")\n",
        "\n",
        "              ax_output.imshow(example_outputs[i][0])\n",
        "              ax_output.set_title(\"Output\")\n",
        "              ax_output.axis(\"off\")\n",
        "\n",
        "              ax_target.imshow(example_targets[i][0])\n",
        "              ax_target.set_title(\"Target\")\n",
        "              ax_target.axis(\"off\")\n",
        "\n",
        "          #plt.tight_layout()\n",
        "          #plt.savefig(f\"Colab Notebooks/Thesis/epochs/predictions_epoch_{epoch+1}.png\")\n",
        "          #plt.close(fig)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_file = f'unet_{num_epochs}epoch_{image_width}x{image_height}.pt'\n",
        "\n",
        "if not os.path.exists(f'{path_main}/models'):\n",
        "  os.makedirs(f'{path_main}/models')\n",
        "\n",
        "torch.save(unet, datapath.joinpath(f'{path_main}/models/{model_file}'))\n",
        "\n",
        "\n",
        "# End the timer\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "print(f\"Elapsed time: {elapsed_time} seconds\")\n"
      ],
      "metadata": {
        "id": "cWPBLpxN16ZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move tensors to CPU memory\n",
        "ious_cpu = [iou.cpu().numpy() for iou in ious]"
      ],
      "metadata": {
        "id": "VfGIrhli2ArE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting\n",
        "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Plot training and validation loss\n",
        "axs[0].plot(range(1, num_epochs+1), train_losses, label='Train Loss')\n",
        "axs[0].plot(range(1, num_epochs+1), validation_losses, label='Validation Loss')\n",
        "axs[0].set_xlabel('Epoch')\n",
        "axs[0].set_ylabel('Loss')\n",
        "axs[0].set_title('Training and Validation Loss')\n",
        "axs[0].legend()\n",
        "\n",
        "# Plot accuracy\n",
        "axs[1].plot(range(1, num_epochs+1), accuracies, label='Accuracy')\n",
        "axs[1].set_xlabel('Epoch')\n",
        "axs[1].set_ylabel('Accuracy')\n",
        "axs[1].set_title('Accuracy over Epochs')\n",
        "\n",
        "# Plot IoU\n",
        "axs[2].plot(range(1, num_epochs+1), ious_cpu, label='IoU')\n",
        "axs[2].set_xlabel('Epoch')\n",
        "axs[2].set_ylabel('IoU')\n",
        "axs[2].set_title('IoU over Epochs')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kyesZ8lr2DIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unet = torch.load(datapath.joinpath(f'{path_main}/models/unet_{num_epochs}epoch_{image_width}x{image_height}.pt'))"
      ],
      "metadata": {
        "id": "KVJ1NADR2Hrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_iter = iter(validation_loader)\n",
        "test_batch, test_label = next(val_iter)"
      ],
      "metadata": {
        "id": "U23m4R462OdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pred = unet(test_batch)\n",
        "\n",
        "# Move the model to the device\n",
        "unet = unet.to(device)\n",
        "\n",
        "# Move the test batch to the device\n",
        "test_batch = test_batch.to(device)\n",
        "\n",
        "# Make the prediction\n",
        "pred = unet(test_batch)"
      ],
      "metadata": {
        "id": "7Vu3YgK92QPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_nr = 10\n",
        "\n",
        "pred_conv = pred[sample_nr]\n",
        "pred_conv = pred_conv.detach().cpu().numpy()\n",
        "pred_conv = pred_conv.reshape(pred_conv.shape[1:])\n",
        "\n",
        "test_conv = test_batch[sample_nr]\n",
        "test_conv = test_conv.permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "label_conv = test_label[sample_nr]\n",
        "label_conv = label_conv.detach().cpu().numpy()\n",
        "label_conv = label_conv.reshape(label_conv.shape[1:])\n",
        "\n",
        "fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n",
        "ax[0].imshow(test_conv)\n",
        "ax[1].imshow(label_conv)\n",
        "ax[2].imshow(pred_conv)"
      ],
      "metadata": {
        "id": "ex7USct32SS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1,3, figsize=(10, 5))\n",
        "\n",
        "# Plot the histogram of the training image\n",
        "ax[0].hist(test_conv.flatten(), bins=256, color='b', alpha=0.5)\n",
        "ax[0].set_xlabel('Pixel Value')\n",
        "ax[0].set_ylabel('Frequency')\n",
        "ax[0].set_title('Histogram of Training Image')\n",
        "\n",
        "# Plot the histogram of the label image\n",
        "ax[1].hist(label_conv.flatten(), bins=256, color='r', alpha=0.5)\n",
        "ax[1].set_xlabel('Pixel Value')\n",
        "ax[1].set_ylabel('Frequency')\n",
        "ax[1].set_title('Histogram of Label Image')\n",
        "\n",
        "# Plot the histogram of the label image\n",
        "ax[2].hist(pred_conv.flatten(), bins=256, color='r', alpha=0.5)\n",
        "ax[2].set_xlabel('Pixel Value')\n",
        "ax[2].set_ylabel('Frequency')\n",
        "ax[2].set_title('Histogram of Predicted Label')"
      ],
      "metadata": {
        "id": "0zcDXkCw2ULM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# Path to the directory containing the original images\n",
        "image_folder = \"yolov5_train/detect_test\"\n",
        "\n",
        "# Path to the directory containing the cropped images\n",
        "output_folder = \"cropped_images\"\n",
        "\n",
        "# Randomly select 5 image pairs\n",
        "image_files = os.listdir(image_folder)\n",
        "random_images = random.sample(image_files, 5)\n",
        "\n",
        "# Create a figure to display the image pairs\n",
        "fig, axs = plt.subplots(5, 2, figsize=(10, 20))\n",
        "\n",
        "# Process each randomly selected image pair\n",
        "for i, image_file in enumerate(random_images):\n",
        "    image_path = os.path.join(image_folder, image_file)\n",
        "    cropped_image_name = image_file[:-4] + \"_0.jpg\"\n",
        "    cropped_image_path = os.path.join(output_folder, cropped_image_name)\n",
        "\n",
        "    # Load the original image\n",
        "    original_image = Image.open(image_path)\n",
        "\n",
        "    # Load and display the cropped image if it exists\n",
        "    if os.path.exists(cropped_image_path):\n",
        "        cropped_image = Image.open(cropped_image_path)\n",
        "\n",
        "        # Display the original image\n",
        "        axs[i, 0].imshow(original_image)\n",
        "        axs[i, 0].axis(\"off\")\n",
        "        axs[i, 0].set_title(\"Original Image\")\n",
        "\n",
        "        # Display the cropped image\n",
        "        axs[i, 1].imshow(cropped_image)\n",
        "        axs[i, 1].axis(\"off\")\n",
        "        axs[i, 1].set_title(\"Cropped Image\")\n",
        "    else:\n",
        "        axs[i, 0].axis(\"off\")\n",
        "        axs[i, 0].set_title(\"Original Image (Not Found)\")\n",
        "        axs[i, 1].axis(\"off\")\n",
        "        axs[i, 1].set_title(\"Cropped Image (Not Found)\")\n",
        "\n",
        "# Adjust the spacing between subplots\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the figure\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pS9QIuw56SKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Path to the directory containing the original images\n",
        "image_folder = \"yolov5_train/detect_test\"\n",
        "\n",
        "# Path to the directory containing the cropped images\n",
        "output_folder = \"cropped_images\"\n",
        "\n",
        "# Path to the UNet model\n",
        "unet_model_path = f'{path_main}/models/unet_{num_epochs}epoch_{image_width}x{image_height}.pt'\n",
        "\n",
        "# Randomly select 5 image pairs\n",
        "image_files = os.listdir(image_folder)\n",
        "random_images = random.sample(image_files, 5)\n",
        "\n",
        "# Load the UNet model\n",
        "unet = Unet(learning_rate=0.001)  # Replace with the appropriate UNet model class\n",
        "unet = torch.load(unet_model_path)\n",
        "unet = unet.to(device)\n",
        "unet.eval()\n",
        "\n",
        "\n",
        "desired_width = image_width\n",
        "desired_height = image_height\n",
        "\n",
        "\n",
        "# Create a figure to display the image pairs\n",
        "fig, axs = plt.subplots(5, 3, figsize=(15, 20))  # Update to 3 columns\n",
        "\n",
        "# Process each randomly selected image pair\n",
        "for i, image_file in enumerate(random_images):\n",
        "    image_path = os.path.join(image_folder, image_file)\n",
        "    cropped_image_name = image_file[:-4] + \"_0.jpg\"\n",
        "    cropped_image_path = os.path.join(output_folder, cropped_image_name)\n",
        "\n",
        "    # Load the original image\n",
        "    original_image = Image.open(image_path)\n",
        "    original_image = Resize((desired_width, desired_height))(original_image)\n",
        "    original_tensor = ToTensor()(original_image).unsqueeze(0).to(device)\n",
        "\n",
        "    # Load and display the cropped image if it exists\n",
        "    if os.path.exists(cropped_image_path):\n",
        "        cropped_image = Image.open(cropped_image_path)\n",
        "        cropped_image = Resize((desired_width, desired_height))(cropped_image)\n",
        "        cropped_tensor = ToTensor()(cropped_image).unsqueeze(0).to(device)\n",
        "\n",
        "        # Make the UNet prediction on the cropped image\n",
        "        with torch.no_grad():\n",
        "            cropped_pred = unet(cropped_tensor)\n",
        "\n",
        "        # Convert the predicted tensor to an image\n",
        "        cropped_pred_image = ToPILImage()(cropped_pred.squeeze(0).cpu())\n",
        "\n",
        "        # Display the original image, cropped image, and predicted image\n",
        "        axs[i, 0].imshow(original_image)\n",
        "        axs[i, 0].axis(\"off\")\n",
        "        axs[i, 0].set_title(\"Original Image\")\n",
        "\n",
        "        axs[i, 1].imshow(cropped_image)\n",
        "        axs[i, 1].axis(\"off\")\n",
        "        axs[i, 1].set_title(\"Cropped Image\")\n",
        "\n",
        "        axs[i, 2].imshow(cropped_pred_image)\n",
        "        axs[i, 2].axis(\"off\")\n",
        "        axs[i, 2].set_title(\"Predicted Image\")\n",
        "    else:\n",
        "        axs[i, 0].axis(\"off\")\n",
        "        axs[i, 0].set_title(\"Original Image (Not Found)\")\n",
        "        axs[i, 1].axis(\"off\")\n",
        "        axs[i, 1].set_title(\"Cropped Image (Not Found)\")\n",
        "\n",
        "# Remove empty subplots if the number of random images is less than 5\n",
        "for j in range(len(random_images), 5):\n",
        "    axs[j, 0].axis(\"off\")\n",
        "    axs[j, 1].axis(\"off\")\n",
        "    axs[j, 2].axis(\"off\")\n",
        "\n",
        "# Adjust the spacing between subplots\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the figure\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lPT-NykhAklF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}